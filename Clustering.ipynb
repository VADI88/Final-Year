{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\A0134451N\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from linear_algebra import squared_distance, vector_mean, distance\n",
    "import math, random\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"performs k-means clustering\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k          # number of clusters\n",
    "        self.means = None   # means of clusters\n",
    "        \n",
    "    def classify(self, input):\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "                   \n",
    "    def train(self, inputs):\n",
    "    \n",
    "        self.means = random.sample(inputs, self.k)\n",
    "        assignments = None\n",
    "        \n",
    "        while True:\n",
    "            # Find new assignments\n",
    "            new_assignments = map(self.classify, inputs)\n",
    "\n",
    "            # If no assignments have changed, we're done.\n",
    "            if assignments == new_assignments:                \n",
    "                return\n",
    "\n",
    "            # Otherwise keep the new assignments,\n",
    "            assignments = new_assignments    \n",
    "\n",
    "            for i in range(self.k):\n",
    "                i_points = [p for p, a in zip(inputs, assignments) if a == i]\n",
    "                # avoid divide-by-zero if i_points is empty\n",
    "                if i_points:                                \n",
    "                    self.means[i] = vector_mean(i_points)    \n",
    "\n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = map(clusterer.classify, inputs)\n",
    "    \n",
    "    return sum(squared_distance(input,means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def plot_squared_clustering_errors(plt):\n",
    "\n",
    "    ks = range(1, len(inputs) + 1)\n",
    "    errors = [squared_clustering_errors(inputs, k) for k in ks]\n",
    "\n",
    "    plt.plot(ks, errors)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"total squared error\")\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# using clustering to recolor an image\n",
    "#\n",
    "\n",
    "def recolor_image(input_file, k=5):\n",
    "\n",
    "    img = mpimg.imread(path_to_png_file)\n",
    "    pixels = [pixel for row in img for pixel in row]\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(pixels) # this might take a while    \n",
    "\n",
    "    def recolor(pixel):\n",
    "        cluster = clusterer.classify(pixel) # index of the closest cluster\n",
    "        return clusterer.means[cluster]     # mean of the closest cluster\n",
    "\n",
    "    new_img = [[recolor(pixel) for pixel in row]\n",
    "               for row in img]\n",
    "\n",
    "    plt.imshow(new_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# hierarchical clustering\n",
    "#\n",
    "\n",
    "def is_leaf(cluster):\n",
    "    \"\"\"a cluster is a leaf if it has length 1\"\"\"\n",
    "    return len(cluster) == 1\n",
    "\n",
    "def get_children(cluster):\n",
    "    \"\"\"returns the two children of this cluster if it's a merged cluster;\n",
    "    raises an exception if this is a leaf cluster\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        raise TypeError(\"a leaf cluster has no children\")\n",
    "    else:\n",
    "        return cluster[1]\n",
    "\n",
    "def get_values(cluster):\n",
    "    \"\"\"returns the value in this cluster (if it's a leaf cluster)\n",
    "    or all the values in the leaf clusters below it (if it's not)\"\"\"\n",
    "    if is_leaf(cluster):\n",
    "        return cluster # is already a 1-tuple containing value\n",
    "    else:\n",
    "        return [value\n",
    "                for child in get_children(cluster)\n",
    "                for value in get_values(child)]\n",
    "\n",
    "def cluster_distance(cluster1, cluster2, distance_agg=min):\n",
    "    \"\"\"finds the aggregate distance between elements of cluster1\n",
    "    and elements of cluster2\"\"\"\n",
    "    return distance_agg([distance(input1, input2)\n",
    "                        for input1 in get_values(cluster1)\n",
    "                        for input2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if is_leaf(cluster):\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return cluster[0] # merge_order is first element of 2-tuple\n",
    "\n",
    "def bottom_up_cluster(inputs, distance_agg=min):\n",
    "    # start with every input a leaf cluster / 1-tuple\n",
    "    clusters = [(input,) for input in inputs]\n",
    "    \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min([(cluster1, cluster2)\n",
    "                     for i, cluster1 in enumerate(clusters)\n",
    "                     for cluster2 in clusters[:i]],\n",
    "                     key=lambda (x, y): cluster_distance(x, y, distance_agg))\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        import numpy\n",
    "        clusters = [c for c in clusters if numpy.all(c != c1) and numpy.all(c != c2)]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = (len(clusters), [c1, c2])\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster, num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "    \n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Final_working.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=df.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_scaled=preprocessing.scale(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " errors as a function of k\n",
      "1 1569800.0\n",
      "2 1489676.80111\n",
      "3 1454781.60955\n",
      "4 1429966.46822\n",
      "5 1407096.81697\n",
      "6 1386895.6391\n",
      "7 1369107.96355\n",
      "8 1353792.26602\n",
      "9 1341740.61713\n",
      "10 1313165.37905\n",
      "11 1294350.13243\n",
      "12 1270212.66825\n",
      "13 1259804.54995\n",
      "14 1248989.5135\n",
      "15 1224649.56335\n",
      "16 1216973.35575\n",
      "17 1207633.89837\n",
      "18 1172499.06477\n",
      "19 1177130.06563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"errors as a function of k\"\n",
    "for k in range(1, 20):\n",
    "    print k, squared_clustering_errors(inputs_scaled, k)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors=[[1, 1569800.0]\n",
    ",[2 ,1489676.80111]\n",
    ",[3 ,1454781.60955]\n",
    ",[4, 1429966.46822]\n",
    ",[5, 1407096.81697]\n",
    ",[6, 1386895.6391]\n",
    ",[7, 1369107.96355]\n",
    ",[8, 1353792.26602]\n",
    ",[9, 1341740.61713]\n",
    ",[10, 1313165.37905]\n",
    ",[11, 1294350.13243]\n",
    ",[12, 1270212.66825]\n",
    ",[13, 1259804.54995]\n",
    ",[14, 1248989.5135]\n",
    ",[15, 1224649.56335]\n",
    ",[16, 1216973.35575]\n",
    ",[17, 1207633.89837]\n",
    ",[18, 1172499.06477]\n",
    ",[19, 1177130.06563]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEPCAYAAAB7rQKTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18cc2898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "y=[1569800,\n",
    "1489676.801,\n",
    "1454781.61,\n",
    "1429966.468,\n",
    "1407096.817,\n",
    "1386895.639,\n",
    "1369107.964,\n",
    "1353792.266,\n",
    "1341740.617,\n",
    "1313165.379,\n",
    "1294350.132,\n",
    "1270212.668,\n",
    "1259804.55,\n",
    "1248989.514,\n",
    "1224649.563,\n",
    "1216973.356,\n",
    "1207633.898,\n",
    "1172499.065,\n",
    "1177130.066]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(x,y)\n",
    "plt.xticks(x)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"total squared error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster.k_means_:\n",
      "\n",
      "class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin)\n",
      " |  K-Means clustering\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, optional, default: 8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  max_iter : int, default: 300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  n_init : int, default: 10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  init : {'k-means++', 'random' or an ndarray}\n",
      " |      Method for initialization, defaults to 'k-means++':\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances\n",
      " |  \n",
      " |      False : never precompute distances\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Relative tolerance with regards to inertia to declare convergence\n",
      " |  \n",
      " |  n_jobs : int\n",
      " |      The number of jobs to use for the computation. This works by computing\n",
      " |      each of the n_init runs in parallel.\n",
      " |  \n",
      " |      If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      " |      used at all, which is useful for debugging. For n_jobs below -1,\n",
      " |      (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      " |      are used.\n",
      " |  \n",
      " |  random_state : integer or numpy.RandomState, optional\n",
      " |      The generator used to initialize the centers. If an integer is\n",
      " |      given, it fixes the seed. Defaults to the global numpy random\n",
      " |      number generator.\n",
      " |  \n",
      " |  verbose : int, default 0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  copy_x : boolean, default True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first.  If copy_x is True, then the original data is not\n",
      " |      modified.  If False, the original data is modified, and put back before\n",
      " |      the function returns, but small numerical differences may be introduced\n",
      " |      by subtracting and then adding the data mean.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : array, [n_clusters, n_features]\n",
      " |      Coordinates of cluster centers\n",
      " |  \n",
      " |  labels_ :\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  Notes\n",
      " |  ------\n",
      " |  The k-means problem is solved using Lloyd's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), were n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  \n",
      " |  MiniBatchKMeans:\n",
      " |      Alternative online implementation that does incremental updates\n",
      " |      of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster to than the default batch implementation.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1)\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X, y=None)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array, shape [n_samples, k]\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_clusters = 11\n",
    "km1 = KMeans(n_clusters=num_clusters, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=11, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=23, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time km1.fit(inputs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters1 = km1.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 0,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 10,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 7,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\a0134451n\\appdata\\local\\continuum\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18498763454632486"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.array(clusters1)\n",
    "silhouette_score(inputs_scaled, array1, metric='euclidean', sample_size=None, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.02746252e-01,  -7.95799693e-02,   7.39664768e-01, ...,\n",
       "         -3.35854575e-01,   4.60314858e-01,   3.05461499e-03],\n",
       "       [  2.84374758e-02,   6.52158509e-02,   5.43218013e-01, ...,\n",
       "         -1.34149431e+00,   4.31084536e-01,  -6.00105036e-03],\n",
       "       [  1.31745702e-02,  -3.39860385e-02,   4.57500739e-01, ...,\n",
       "         -2.94284184e-01,   4.03009248e-01,  -4.87458696e-02],\n",
       "       ..., \n",
       "       [  1.34778132e-01,  -3.50608043e-01,   1.26392656e+00, ...,\n",
       "          2.92292706e-01,   2.07039835e-01,  -4.23627029e-01],\n",
       "       [ -8.22883618e-02,  -9.74343884e-01,   3.52412214e+00, ...,\n",
       "          2.92292706e-01,   6.59701579e-01,  -9.60574716e-01],\n",
       "       [ -3.75360165e-02,   2.48173757e-02,  -1.05870580e-02, ...,\n",
       "         -4.13113947e-01,   3.15230966e-01,   3.99664159e-02]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km1.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Cluster']=km1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1        10\n",
       "2        10\n",
       "3         4\n",
       "4         3\n",
       "5         4\n",
       "6         4\n",
       "7        10\n",
       "8        10\n",
       "9        10\n",
       "10        2\n",
       "11       10\n",
       "12       10\n",
       "13        2\n",
       "14        0\n",
       "15       10\n",
       "16        2\n",
       "17        4\n",
       "18       10\n",
       "19        4\n",
       "20       10\n",
       "21        4\n",
       "22       10\n",
       "23        4\n",
       "24       10\n",
       "25        4\n",
       "26        2\n",
       "27        7\n",
       "28       10\n",
       "29        4\n",
       "         ..\n",
       "15668     4\n",
       "15669     4\n",
       "15670     4\n",
       "15671     4\n",
       "15672     4\n",
       "15673     4\n",
       "15674     4\n",
       "15675     4\n",
       "15676     4\n",
       "15677     4\n",
       "15678     4\n",
       "15679     4\n",
       "15680     4\n",
       "15681     4\n",
       "15682     4\n",
       "15683     4\n",
       "15684     4\n",
       "15685     4\n",
       "15686     4\n",
       "15687     4\n",
       "15688     4\n",
       "15689     4\n",
       "15690     4\n",
       "15691     4\n",
       "15692     4\n",
       "15693     4\n",
       "15694     4\n",
       "15695     4\n",
       "15696     4\n",
       "15697     4\n",
       "Name: Cluster, dtype: int32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Cluster_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6\n",
    "km2 = KMeans(n_clusters=num_clusters, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=6, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=23, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time km2.fit(inputs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    10210\n",
      "0     4069\n",
      "1      831\n",
      "2      560\n",
      "5       27\n",
      "4        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "clusters2 = km2.labels_.tolist()\n",
    "ps = pandas.Series([i for i in clusters2])\n",
    "counts = ps.value_counts()\n",
    "print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
